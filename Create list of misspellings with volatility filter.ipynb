{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fee1fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "#import urllib.parse\n",
    "#import unicodedata\n",
    "import csv\n",
    "import itertools\n",
    "from pattern.text.nl import singularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "174aa664",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'C:\\\\Users\\\\KoeReu\\\\OneDrive - Blokker B.V\\\\Bureaublad\\\\Google BQ\\\\universal-analytics-nextail-baa80f5ed3cc.json'\n",
    "\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce350ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of all letters, numbers and a space (\" \")\n",
    "all_letters = list('abcdefghijklmnopqrstuvqxyz1234567890 ')\n",
    "\n",
    "# create list of tonal changes in Dutch\n",
    "tone_change_lst = [('eau', 'o'), ('eau', 'oo'), ('ij', 'ei'), ('ei', 'ij'), ('ij', 'y'), ('ei', 'y'), \n",
    "                   ('y', 'ei'), ('y', 'ij'), ('au', 'ou'), ('ou', 'au'), ('g', 'ch'), ('ch', 'g'), \n",
    "                   ('ph', 'f'), ('sch', 'sg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc3daac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_possible_misspellings(string: str):\n",
    "    \n",
    "    \"\"\"Takes in a word and returns all possible misspellings in a set. This is done by the actions below. Input stofzuiger\n",
    "    returns:\n",
    "    - Removing letters - tofzuiger, sofzuiger, stfzuiger, stozuiger etc.\n",
    "    - Repeating letters - sstofzuiger, sttofzuiger, stoofzuiger, stoffzuiger etc.\n",
    "    - Creating typos - atofzuiger, btofzuiger, ctofzuiger, saofzuiger etc.\n",
    "    - Switching adjacent letters - tsofzuiger, sotfzuiger, stfozuiger etc.\n",
    "    - Rewriting tone changes - input airfryer returns airfrijer, airfreier etc.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    string_lst = list(string)\n",
    "    misspellings = []\n",
    "                   \n",
    "    for num, letter in enumerate(string_lst):\n",
    "        \n",
    "        # removal of letters\n",
    "        misspellings.append(''.join(string_lst[:num] + string_lst[num + 1:]).strip())\n",
    "        \n",
    "        # repetition of letters\n",
    "        misspellings.append(''.join(string_lst[:num] + [letter] * 2 + string_lst[num + 1:]))\n",
    "        \n",
    "        # create typos\n",
    "        for repl_letter in all_letters:\n",
    "            misspellings.append(''.join(string_lst[:num] + [repl_letter] + string_lst[num + 1:]).strip())\n",
    "    \n",
    "        # switching of adjacent letters\n",
    "        if num == 0:\n",
    "            misspellings.append(''.join([string_lst[num + 1]] + [string_lst[num]] + string_lst[num + 2:]))\n",
    "        elif num < len(string_lst) - 1:\n",
    "            misspellings.append(''.join(string_lst[:num] + [string_lst[num + 1]] + [string_lst[num]] + string_lst[num + 2:]))\n",
    "            \n",
    "    # change tone changes in Dutch (such as au: ou and ei: ij)\n",
    "    for row in tone_change_lst:\n",
    "        letter = row[0]\n",
    "        repl_letter = row[1]\n",
    "        misspellings.append(string.replace(letter, repl_letter))\n",
    "            \n",
    "    return set(misspellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b33b1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vkw(slug):\n",
    "    \n",
    "    \"\"\" Takes in category slug (e.g. stofzuigers or stofzuigers/philips) and returns the singular word and the \n",
    "    vkw (verkleinwoord) in a set (e.g. stofzuiger, stofzuigers, stofzuigertje, stofzuigertjes) \"\"\"\n",
    "    \n",
    "    slug = slug.replace('-', ' ')\n",
    "    #slug = slug.replace('/', ' ')\n",
    "    slug_singular = singularize(slug)\n",
    "    \n",
    "    if re.match('.*pan$', slug_singular):\n",
    "        vkw = slug_singular + 'netje'\n",
    "        \n",
    "    elif re.match('.*glas$', slug_singular):\n",
    "        vkw = re.sub('glas', 'glaasje', slug_singular)\n",
    "        \n",
    "    elif re.match('.*kom$', slug_singular):\n",
    "        vkw = re.sub('kom', 'kommetje', slug_singular)\n",
    "                         \n",
    "    elif re.match('.*m$', slug_singular):\n",
    "        vkw = slug_singular + 'pje'\n",
    "                     \n",
    "    elif re.match('.*[k|t|p|s|d]$', slug_singular):\n",
    "        vkw = slug_singular + 'je'\n",
    "    \n",
    "    elif re.match('.*[n|l|r]$', slug_singular):\n",
    "        vkw = slug_singular + 'tje'\n",
    "    \n",
    "    try:\n",
    "        return {slug, slug_singular, vkw, vkw + 's'}\n",
    "    \n",
    "    except:\n",
    "        return {slug, slug_singular}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f12a2b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filters_from_url(clean_url: str):\n",
    "\n",
    "    \"\"\"This function returns all possible singular and plural words and verkleinwoorden for each category.\n",
    "        \n",
    "    URL /stofzuigers returns\n",
    "    - stofzuiger\n",
    "    - stofzuigers\n",
    "    - stofzuigertje\n",
    "    - stofzuigertjes\n",
    "    \n",
    "    URL /stofzuigers/philips returns:\n",
    "    - philips stofzuiger\n",
    "    - philips stofzuigers\n",
    "    - philips stofzuigertje\n",
    "    - philips stofzuigertjes\n",
    "    - stofzuiger philips\n",
    "    - stofzuigers philips\n",
    "    - stofzuigertje philips\n",
    "    - stofzuigertjes philips\n",
    "    \"\"\"\n",
    "        \n",
    "    # split clean URL from character two and beyond on slash (/) and underscore (_)\n",
    "    splitted_clean_url = re.split('[\\/_]', clean_url[1:])\n",
    "    all_words = vkw(splitted_clean_url[0])\n",
    "    \n",
    "    if len(splitted_clean_url) == 1:\n",
    "        return all_words\n",
    "    \n",
    "    elif len(splitted_clean_url) >= 2:\n",
    "        r_set = set()\n",
    "        r = itertools.chain(itertools.product(all_words, splitted_clean_url[1:]), itertools.product(splitted_clean_url[1:], all_words))\n",
    "        for a, b in r:\n",
    "            r_set.add(f'{a} {b}')\n",
    "            \n",
    "        return r_set\n",
    "        \n",
    "    # else: add third and beyond options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdefc4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_misspellings(clean_url: str):\n",
    "    \"\"\"\n",
    "    This function takes in a clean URL and returns all possible misspellings.\n",
    "    \n",
    "    URL /stofzuigers returns:\n",
    "    - smofzuiger\n",
    "    - stofzuigeas\n",
    "    - st0fzuiger\n",
    "    - atofzuigers\n",
    "    - stof6uigers\n",
    "    \n",
    "    and 1,800 other possible misspellings\n",
    "    \"\"\" \n",
    "    \n",
    "    misspellings = []\n",
    "    \n",
    "    for wrd in extract_filters_from_url(clean_url):\n",
    "        \n",
    "        misspellings.extend(all_possible_misspellings(wrd))\n",
    "        \n",
    "    return set(misspellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abcab00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to change period here!\n",
    "\n",
    "QUERY = (\n",
    "    r\"\"\"\n",
    "DECLARE re_str_brands STRING DEFAULT (SELECT CONCAT('/(', STRING_AGG(SUBSTRING(brand_slug, 2), '|'), ')$') FROM `universal-analytics-nextail.brands.brands_and_brand_slugs`);\n",
    "\n",
    "SELECT LOWER(REGEXP_EXTRACT((SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'clean_url'), '(/[^/]+)')),\n",
    "COUNT(*)\n",
    "FROM `universal-analytics-nextail.analytics_283141299.events_*` \n",
    "WHERE --_TABLE_SUFFIX BETWEEN '20240601' AND FORMAT_DATE('%Y%m%d', CURRENT_DATE()) AND\n",
    "event_name = 'page_view' AND \n",
    "(SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_type') = 'category' AND\n",
    "NOT REGEXP_CONTAINS((SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'clean_url'), '^\\\\/(wk\\\\d|winter|bc|douwe|trend|folder)') AND\n",
    "NOT REGEXP_CONTAINS((SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'clean_url'), re_str_brands) AND\n",
    "(SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'clean_url') NOT LIKE '%-en-%'\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "\n",
    "    \"\"\"\n",
    "        )\n",
    "\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8947d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_dict = {}\n",
    "\n",
    "for clean_url, pv_count in query_job:\n",
    "    if clean_url :\n",
    "        og_dict[clean_url] = pv_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "466db25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for k, v in og_dict.items():\n",
    "    if len(k) > 3:\n",
    "        result_dict[k] = create_misspellings(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e2620f",
   "metadata": {},
   "source": [
    "#### Read Nikki's redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c142dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KoeReu\\OneDrive - Blokker B.V\\Documenten\\Nikkis search redirects\n"
     ]
    }
   ],
   "source": [
    "# Read Nikki's redirects\n",
    "%cd C:\\Users\\KoeReu\\OneDrive - Blokker B.V\\Documenten\\Nikkis search redirects\n",
    "\n",
    "sd = []\n",
    "\n",
    "with open('search_driven_redirects.csv', mode = 'r', newline = '\\n') as f:\n",
    "    csv_reader = csv.reader(f, delimiter = ';')\n",
    "    next(csv_reader) # skip first row\n",
    "    # columns ['Categorie', 'Cat. ID', 'Live per', 'Zoekwoorden', '']\n",
    "    \n",
    "    for cat, cat_id, date, _, search_queries in csv_reader:\n",
    "        \n",
    "        sq = re.split(',\\s?', search_queries)\n",
    "        sd.append([cat, cat_id, [x.strip() for x in sq]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "090b6ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9547\n"
     ]
    }
   ],
   "source": [
    "# unique redirects to all url's (e.g. prullenbakdn, prullenbak, prullenbakken, prullenbk, prullenbakk etc.)\n",
    "search_redirects = set(list(itertools.chain.from_iterable([x[2] for x in sd])))\n",
    "print(len(search_redirects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96305bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of erractic search behaviour was found in the dataset. This seemed to be generated by bot traffic, where\n",
    "# obscure (not natural looking) search queries were triggered in high volumes. Another trait was that these search queries\n",
    "# were highly volatile by nature. In the query below, all search queries per day are cross joined with all dates. This way,\n",
    "# search queries with high volatility can be disinguished and possibly ignored.\n",
    "\n",
    "\n",
    "QUERY = (\n",
    "    r\"\"\"\n",
    "DECLARE start_date DATE DEFAULT '2024-02-01';\n",
    "DECLARE end_date DATE DEFAULT DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY);\n",
    "\n",
    "WITH cte AS (SELECT event_date, \n",
    "TRIM((SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'search_term')) AS search_term,\n",
    "COUNT(*) AS search_volume\n",
    "FROM `universal-analytics-nextail.analytics_283141299.events_*`\n",
    "WHERE _TABLE_SUFFIX BETWEEN FORMAT_DATE('%Y%m%d', start_date) AND \n",
    "FORMAT_DATE('%Y%m%d', end_date) AND\n",
    "event_name = 'interaction_search' AND\n",
    "(SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'search_type') = 'standard_search'\n",
    "GROUP BY 1, 2),\n",
    "\n",
    "cte_2 AS (SELECT * -- dates, cte.search_term, cte.search_volume\n",
    "FROM UNNEST(GENERATE_DATE_ARRAY(start_date, end_date, INTERVAL 1 DAY)) AS dates\n",
    "CROSS JOIN (SELECT DISTINCT(search_term) FROM cte)),\n",
    "\n",
    "cte_3 AS (SELECT cte_2.dates, cte_2.search_term, COALESCE(cte.search_volume, 0) AS sv FROM cte_2\n",
    "LEFT JOIN cte ON cte_2.dates = PARSE_DATE('%Y%m%d', cte.event_date) AND cte_2.search_term = cte.search_term)\n",
    "\n",
    "SELECT cte_3.search_term,\n",
    "SUM(sv) AS sv_sum,\n",
    "AVG(sv) AS sv_avg,\n",
    "STDDEV(sv) AS sv_std,\n",
    "DATE_DIFF(end_date, start_date, DAY) AS delta_days, \n",
    "SQRT(DATE_DIFF(end_date, start_date, DAY)) AS sqrt_dd,\n",
    "STDDEV(sv) * SQRT(DATE_DIFF(end_date, start_date, DAY)) AS volatility,\n",
    "CASE WHEN STDDEV(sv) * SQRT(DATE_DIFF(end_date, start_date, DAY)) > 500 THEN true ELSE false END AS highly_volatile\n",
    "FROM cte_3\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "    \"\"\"\n",
    "        )\n",
    "\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows_sq = query_job.result()  # Waits for query to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0634ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_url_redirects = list(itertools.chain.from_iterable(result_dict.values())) # create list of nested lists\n",
    "ga4_sq = {}\n",
    "all_kw = {}\n",
    "sq_lookup_set = set()\n",
    "\n",
    "for search_query, sv_sum, sv_avg, sv_std, delta_days, sqrt_dd, volatility, highly_volatile in query_job:\n",
    "    \n",
    "    if search_query and search_query not in ga4_sq:\n",
    "        ga4_sq[search_query] = 0\n",
    "    \n",
    "    # the dictionary here is filled with search volumes for keywords without high volatility\n",
    "    if search_query and highly_volatile == False:\n",
    "        ga4_sq[search_query] += sv_sum\n",
    "               \n",
    "    if search_query:\n",
    "        all_kw[search_query] = [sv_sum, sv_avg, sv_std, delta_days, sqrt_dd, volatility, highly_volatile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee9e79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list for exporting to CSV\n",
    "to_csv = []\n",
    "min_search_volume = 1 # filter for minimum search volume per query\n",
    "\n",
    "for k, v in result_dict.items():\n",
    "    result_sd = [] # search redirects\n",
    "    result_nsd = [] # search non-directs\n",
    "    \n",
    "    for sq in v:\n",
    "        # search volume excluding high volatility keywords\n",
    "        search_volume = ga4_sq.get(sq, False)\n",
    "        \n",
    "        if search_volume and search_volume >= min_search_volume:\n",
    "            if sq in search_redirects:\n",
    "                result_sd.append((sq, search_volume))\n",
    "                \n",
    "            elif sq not in search_redirects:\n",
    "                result_nsd.append((sq, search_volume))\n",
    "                           \n",
    "    result_sd = sorted(result_sd, key = lambda x: x[1], reverse = True)\n",
    "    result_nsd = sorted(result_nsd, key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    if result_nsd:\n",
    "        to_csv.append([k, result_nsd, result_sd, sum([x[1] for x in result_nsd])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30e6ccac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\KoeReu\\\\OneDrive - Blokker B.V\\\\Documenten\\\\Nikkis search redirects'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e6d053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('potentieel_ontbrekende_redirects_with_self_created_misspellings.csv', 'w', newline = '\\n', encoding = 'utf-8') as f:\n",
    "    csv_writer = csv.writer(f, delimiter = ';')\n",
    "    csv_writer.writerow(['url', 'search_queries_non_redirected', 'search_queries_non_redirected_w_volume', 'search_queries_redirected', 'total_search_volume'])\n",
    "    \n",
    "    for k, result_nsd, result_sd, search_volume in sorted(to_csv, key = lambda x: x[-1], reverse = True):\n",
    "        csv_writer.writerow([k, ', '.join([f'[{x[0]}]' for x in result_nsd]), result_nsd, result_sd, search_volume])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cc2b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 'search_query, sv_sum, sv_avg, sv_std, delta_days, sqrt_dd, volatility, highly_volatile'\n",
    "\n",
    "with open('all_search_queries.csv', 'w', encoding = 'utf-8', newline = '\\n') as f:\n",
    "    csv_writer = csv.writer(f, delimiter = ';')\n",
    "    csv_writer.writerow([x.strip() for x in cols.split(',')])\n",
    "    \n",
    "    for num, (k, v) in enumerate(all_kw.items()):\n",
    "        \n",
    "        # rewrites floats with dot notation to comma notation (1.99 to 1,99)\n",
    "        w = [\"{:.2f}\".format(x).replace('.', ',') if type(x) == float else x for x in v]\n",
    "        \n",
    "        if num > 10000:\n",
    "            break\n",
    "        else:\n",
    "            csv_writer.writerow([k] + w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a1f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
